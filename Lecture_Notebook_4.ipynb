{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamillaknudsen/tmdl/blob/main/Lecture_Notebook_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edbc2ffd",
      "metadata": {
        "id": "edbc2ffd"
      },
      "source": [
        "# Turing Machine and Deep Learning 2025\n",
        "_Author: Satchit Chatterji (satchit.chatterji@gmail.com)_\n",
        "\n",
        "## Lecture 4 -- Neural Network Playground\n",
        "> Today's question: **How do NNs work?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db438675",
      "metadata": {
        "id": "db438675"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, SimpleRNN"
      ],
      "metadata": {
        "id": "jcUY-l2v1RY6"
      },
      "id": "jcUY-l2v1RY6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16cecdf0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16cecdf0",
        "outputId": "08898cf2-e734-4073-e234-3d57db7753d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load the CIFAR-10 dataset\n",
        "cifar10 = keras.datasets.cifar10\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c17297f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c17297f9",
        "outputId": "de3a2e25-61c4-45b4-9430-3a560143b4d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Artificial Neural Network\n",
        "The easiest way to use a Neural Network, is the Artificial Neural Network (ANN). For ANN's you do not have to manually \"build\" the network yourself. The ```MLPClassifier``` is such an artificial neural network.\n",
        "\n",
        "The MLPClassifier starts with an input layer, taking in the data.\n",
        "Then uses (multiple) hidden layers to attempt to analyze the data and recognize patterns.\n",
        "And finally it has an output layer that converts these findings to classifications, with only one neuron for binary classification (generates the probability score) and for multi-class classification there are as many neurons as classes, with a generated probability score for each class.\n",
        "\n",
        "MLPClassifier has a number of hyperparameters, the ones we use here include:\n",
        "\n",
        "```hidden_layer_sizes``` The number of neurons in each hidden layer (here thus one layer with 32 and one with 64 neurons)\n",
        "\n",
        "```max_iter``` the maximum number of iterations for training\n",
        "\n",
        "To get the validation loss, we set ```early_stopping = True```, as this sets the stopping criterion to be the accuracy score, so it gets automatically computed every iteration.\n",
        "The accuracy score is computed on a validation set, whose size is determined by the ```validation_fraction``` (which is 0 by default, and thus has to be  set as well).\n",
        "\n",
        "There are many other (hyper)paramaters to be set, such as different activation functions, or solvers."
      ],
      "metadata": {
        "id": "0r6HW0NN9U-R"
      },
      "id": "0r6HW0NN9U-R"
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the MLPClassifier\n",
        "model = MLPClassifier(hidden_layer_sizes=(32,64), max_iter=500, early_stopping=True, validation_fraction = 0.2)\n",
        "\n",
        "# Reshape the images and train the model\n",
        "train_images_reshaped = train_images.reshape(train_images.shape[0], -1)\n",
        "model.fit(train_images_reshaped, train_labels)\n",
        "\n",
        "# Get the training loss and validation accuracy values from the model\n",
        "train_loss = model.loss_curve_\n",
        "val_acc = model.validation_scores_\n",
        "\n",
        "# Plot the training losses\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(len(train_loss)), train_loss, label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "\n",
        "# Plot the validation accuracies\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(len(val_acc)), val_acc, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Validation Accuracy')\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cV_pD8U29Uqj"
      },
      "id": "cV_pD8U29Uqj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Convolutional Neural Network\n",
        "For problems that work with visual data (Computer Vision) a Convolutional Neural Network (CNN) is used more commonly.\n",
        "\n",
        "A CNN is like an extended version of an ANN. Instead of the hidden layers of the ANN, they use convolutional, pooling and dense layers.\n",
        "\n",
        "####Convolutional layer\n",
        "Our images are represented as a three-dimensional vector. They have a length and width of 32 pixels, and RGB values for each of those pixels. The convolutional layer then detects spatial features, such as edges, textures or patterns. It then maps the input to a feature map.\n",
        "\n",
        "####Pooling layer\n",
        "The pooling layer then reduces the size of these feature maps by using max pooling; taking the maximum value per region. This reduces the computational cost and makes the model more robust.\n",
        "\n",
        "####Dense layer\n",
        "The Dense layer then converts the extracted features into a final classification decision."
      ],
      "metadata": {
        "id": "KGdppUBw9Y03"
      },
      "id": "KGdppUBw9Y03"
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_images, train_labels, epochs=20, batch_size=32, validation_data=(test_images, test_labels))\n",
        "\n",
        "# Get the training and validation loss values from the history object\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Get the training and validation accuracy values from the history object\n",
        "train_acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "# Plot the training and validation losses\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(len(train_loss)), train_loss, label='Training Loss')\n",
        "plt.plot(range(len(val_loss)), val_loss, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot the training and validation accuracies\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(len(train_acc)), train_acc, label='Training Accuracy')\n",
        "plt.plot(range(len(val_acc)), val_acc, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NPbE3T3h9YTP"
      },
      "id": "NPbE3T3h9YTP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Recurrent Neural Network\n",
        "Recurrent Neural Networks (RNN's) function a bit differently. While ANN and CNN are feed-forward models (they only pass the information forward), RNN's also pass information backwards. They do this by saving the output of the processing nodes and feeding this back into the model. An often-used example for this is predicting words in sentences; in order to predict what word comes next the RNN doesn't just look at the current word, but also takes into account the words that came before it.\n",
        "\n",
        "An RNN uses mostly the same layers as an CNN. An RNN model is built up out of Recurrent Layers, which thus reuse data, (optionally Pooling Layers) and Dense Layers.\n",
        "\n",
        "RNN's are generally used for natural language processing and other text-related purposes, the visual data we analyze here is thus not ideal."
      ],
      "metadata": {
        "id": "24wDwWyZ9gx_"
      },
      "id": "24wDwWyZ9gx_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the RNN model\n",
        "model = Sequential([\n",
        "    SimpleRNN(32, activation='relu', input_shape=(32, 32*3)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Reshape the images and train the model\n",
        "train_images_reshaped = train_images.reshape(train_images.shape[0], 32, 32*3)\n",
        "test_images_reshaped = test_images.reshape(test_images.shape[0], 32, 32*3)\n",
        "history = model.fit(train_images_reshaped, train_labels, epochs=20, batch_size=32, validation_data=(test_images_reshaped, test_labels))\n",
        "\n",
        "# Get the training and validation loss values from the history object\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Get the training and validation accuracy values from the history object\n",
        "train_acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "# Plot the training and validation losses\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(len(train_loss)), train_loss, label='Training Loss')\n",
        "plt.plot(range(len(val_loss)), val_loss, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot the training and validation accuracies\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(len(train_acc)), train_acc, label='Training Accuracy')\n",
        "plt.plot(range(len(val_acc)), val_acc, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZFYTrIPc9mgS"
      },
      "id": "ZFYTrIPc9mgS",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}